# ğŸ§  RT-TM Core Architecture

This document outlines the terminology and architectural philosophy behind the `core` hierarchy in **RT-TM**, our universal tensor backend. These names are more than stylistic choices â€” they reflect distinct **phases** in the tensor execution lifecycle and serve as a foundational abstraction for device scheduling, memory layout, and optimization.

---

## ğŸŒ± `rt_tm::model_core`

**Represents:**
The raw, deserialized operations (nodes) from a model graph.

**Purpose:**

* Parsed directly from the model file (e.g., GGUF, ONNX, etc.)
* Unscheduled and untyped at the C++ level
* Holds metadata like op type, inputs, outputs, attributes

**Think of it as:**

> â€œThe genome of a neural graph before itâ€™s expressed into hardware.â€

**Stored as:**

```cpp
std::vector<model_core>
```

---

## ğŸ”§ `rt_tm::op_core`

**Represents:**
A transformed, validated, typed operation that is ready for execution planning.

**Purpose:**

* Result of compile-time or runtime graph preparation
* Input and output tensor types resolved
* Memory arenas assigned
* Device-neutral execution descriptor

**Think of it as:**

> â€œA ready-to-launch instruction that just needs a CPU or GPU to land on.â€

**Built from:**

```cpp
model_core -> op_core
```

---

## ğŸ§© `rt_tm::cpu_op_core`

**Represents:**
A `op_core` specialized for CPU execution.

**Purpose:**

* Binds CPU-specific memory pools
* Can dispatch into AVX2, AVX-512, NEON, etc.
* Contains platform-specific execution metadata (tiling, prefetch hints, etc.)

**Think of it as:**

> â€œAn op\_core hardened for x86/ARM assault.â€

**Constructed via:**

```cpp
op_core -> cpu_op_core
```

---

## ğŸ”¥ `rt_tm::gpu_op_core`

**Represents:**
A `op_core` mapped to GPU execution targets.

**Purpose:**

* CUDA, Vulkan, or Metal-specific dispatch representation
* Tensors already migrated to GPU memory
* Execution tuned for warp/block/thread layout

**Think of it as:**

> â€œAn op\_core thatâ€™s strapped to a rocket.â€

**Constructed via:**

```cpp
op_core -> gpu_op_core
```

---

## ğŸ§¬ Evolution Flow

```plaintext
model_core
   â†“ deserialization & validation
op_core
   â†“ target binding
cpu_op_core / gpu_op_core
   â†“ device dispatch
execution
```

---

## ğŸ”’ Why â€œcoreâ€?

* **Atomic**: Each `core` represents the indivisible logic of an operation.
* **Minimal**: No fluff. Each layer only carries what it *needs*.
* **Composable**: You can fuse, schedule, or parallelize them efficiently.
* **Semantic**: Clean mental separation between stages of execution.

---

## ğŸ’¡ Summary

The `core` system gives RT-TM:

* Clear delineation of roles across compile/runtime
* A path for constexpr execution in the future
* Perfect-fit abstraction for both CPU and GPU backends

This isnâ€™t just naming.
Itâ€™s **an ontology of inference.**

---
